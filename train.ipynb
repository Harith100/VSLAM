{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.models import resnet34\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Define custom dataset class\n",
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, features_file, positions_file, orientations_file, transform=None):\n",
    "        self.features = torch.load(features_file)\n",
    "        self.positions = torch.load(positions_file)\n",
    "        self.orientations = torch.load(orientations_file)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        features = self.features[idx]\n",
    "        positions = self.positions[idx]\n",
    "        orientations = self.orientations[idx]\n",
    "        \n",
    "        if self.transform:\n",
    "            features = self.transform(features)\n",
    "\n",
    "        return features, positions, orientations\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "class HourglassPose(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(HourglassPose, self).__init__()\n",
    "        \n",
    "        # Load pre-trained ResNet34 as encoder\n",
    "        resnet_model = resnet34(pretrained=True)\n",
    "        self.encoder = nn.Sequential(*list(resnet_model.children())[:-2])  # Remove last avgpool and fc layers\n",
    "        \n",
    "        # Decoder\n",
    "        self.deconv1 = nn.ConvTranspose2d(512, 256, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
    "        self.deconv2 = nn.ConvTranspose2d(256, 128, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
    "        self.deconv3 = nn.ConvTranspose2d(128, 64, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
    "        self.deconv4 = nn.ConvTranspose2d(64, 32, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
    "        self.conv_final = nn.Conv2d(32, 32, kernel_size=3, padding=1)\n",
    "        \n",
    "        # Skip connections\n",
    "        self.skip1 = nn.Conv2d(512, 32, kernel_size=1)\n",
    "        self.skip2 = nn.Conv2d(256, 32, kernel_size=1)\n",
    "        self.skip3 = nn.Conv2d(128, 32, kernel_size=1)\n",
    "        self.skip4 = nn.Conv2d(64, 32, kernel_size=1)\n",
    "        \n",
    "        # Regressor\n",
    "        self.fc_loc = nn.Linear(32 * 56 * 56, 3)\n",
    "        self.fc_ori = nn.Linear(32 * 56 * 56, 4)\n",
    "        self.fc_trans = nn.Linear(32 * 56 * 56, 3)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        x = self.encoder(x)\n",
    "        skip1 = self.skip1(x)\n",
    "        \n",
    "        # Decoder\n",
    "        x = self.deconv1(x)\n",
    "        skip2 = self.skip2(x[:, :256, :, :])  # Ensure correct number of input channels\n",
    "        x = torch.cat((x, skip2), dim=1)\n",
    "        \n",
    "        x = self.deconv2(x)\n",
    "        skip3 = self.skip3(x[:, :128, :, :])  # Ensure correct number of input channels\n",
    "        x = torch.cat((x, skip3), dim=1)\n",
    "        \n",
    "        x = self.deconv3(x)\n",
    "        skip4 = self.skip4(x[:, :64, :, :])  # Ensure correct number of input channels\n",
    "        x = torch.cat((x, skip4, skip1), dim=1)\n",
    "        \n",
    "        x = self.deconv4(x)\n",
    "        x = self.conv_final(x)\n",
    "        \n",
    "        # Regressor\n",
    "        x = x.view(x.size(0), -1)\n",
    "        loc = self.fc_loc(x)\n",
    "        ori = self.fc_ori(x)\n",
    "        trans = self.fc_trans(x)\n",
    "        \n",
    "        return loc, ori, trans\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HourglassPose(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(HourglassPose, self).__init__()\n",
    "        \n",
    "        # Load pre-trained ResNet34 as encoder\n",
    "        resnet_model = resnet34(pretrained=True)\n",
    "        self.encoder = nn.Sequential(*list(resnet_model.children())[:-2])  # Remove last avgpool and fc layers\n",
    "        \n",
    "        # Decoder\n",
    "        self.deconv1 = nn.ConvTranspose2d(512, 256, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
    "        self.deconv2 = nn.ConvTranspose2d(256, 128, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
    "        self.deconv3 = nn.ConvTranspose2d(128, 64, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
    "        self.deconv4 = nn.ConvTranspose2d(64, 32, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
    "        self.conv_final = nn.Conv2d(32, 32, kernel_size=3, padding=1)\n",
    "        \n",
    "        # Skip connections\n",
    "        self.skip1 = nn.Conv2d(512, 32, kernel_size=1)\n",
    "        self.skip2 = nn.Conv2d(256, 128, kernel_size=1)  # Adjusted number of output channels\n",
    "        self.skip3 = nn.Conv2d(128, 64, kernel_size=1)   # Adjusted number of output channels\n",
    "        self.skip4 = nn.Conv2d(64, 32, kernel_size=1)\n",
    "        \n",
    "        # Regressor\n",
    "        self.fc_loc = nn.Linear(32 * 56 * 56, 3)\n",
    "        self.fc_ori = nn.Linear(32 * 56 * 56, 4)\n",
    "        self.fc_trans = nn.Linear(32 * 56 * 56, 3)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        x = self.encoder(x)\n",
    "        skip1 = self.skip1(x)\n",
    "        \n",
    "        # Decoder\n",
    "        x = self.deconv1(x)\n",
    "        skip2 = self.skip2(x)  # Adjusted skip connection\n",
    "        x = torch.cat((x, skip2), dim=1)\n",
    "        \n",
    "        x = self.deconv2(x)\n",
    "        skip3 = self.skip3(x)  # Adjusted skip connection\n",
    "        x = torch.cat((x, skip3), dim=1)\n",
    "        \n",
    "        x = self.deconv3(x)\n",
    "        skip4 = self.skip4(x)\n",
    "        x = torch.cat((x, skip4, skip1), dim=1)\n",
    "        \n",
    "        x = self.deconv4(x)\n",
    "        x = self.conv_final(x)\n",
    "        \n",
    "        # Regressor\n",
    "        x = x.view(x.size(0), -1)\n",
    "        loc = self.fc_loc(x)\n",
    "        ori = self.fc_ori(x)\n",
    "        trans = self.fc_trans(x)\n",
    "        \n",
    "        return loc, ori, trans\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define training parameters\n",
    "batch_size = 32\n",
    "learning_rate = 0.001\n",
    "num_epochs = 10\n",
    "#ft=\"D:/slam/VSLAM/Extracted/Features.pt\"\n",
    "#ps=\"D:/slam/VSLAM/Extracted/Positions.pt\"\n",
    "#ot=\"D:/slam/VSLAM/Extracted/Orientations.pt\"\n",
    "\n",
    "# Instantiate dataset and dataloader\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Resize images to match ResNet input size\n",
    "    #transforms.ToTensor()\n",
    "])\n",
    "dataset = CustomDataset('D:/slam/VSLAM/Extracted/Features.pt', 'D:/slam/VSLAM/Extracted/Positions.pt', 'D:/slam/VSLAM/Extracted/Orientations.pt', transform=transform)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Instantiate model, loss function, and optimizer\n",
    "model = HourglassPose()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = 0.0\n",
    "    for features, positions, orientations in dataloader:\n",
    "        optimizer.zero_grad()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Given transposed=1, weight of size [256, 128, 3, 3], expected input[32, 384, 14, 14] to have 256 channels, but got 384 channels instead",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[41], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m epoch_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m features, positions, orientations \u001b[38;5;129;01min\u001b[39;00m dataloader:\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m     loc_pred, ori_pred, trans_pred \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;66;03m# Compute loss\u001b[39;00m\n\u001b[0;32m      8\u001b[0m     loc_loss \u001b[38;5;241m=\u001b[39m criterion(loc_pred, positions)\n",
      "File \u001b[1;32md:\\slam\\env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\slam\\env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[39], line 37\u001b[0m, in \u001b[0;36mHourglassPose.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     34\u001b[0m skip2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mskip2(x)  \u001b[38;5;66;03m# Adjusted skip connection\u001b[39;00m\n\u001b[0;32m     35\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((x, skip2), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m---> 37\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdeconv2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     38\u001b[0m skip3 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mskip3(x)  \u001b[38;5;66;03m# Adjusted skip connection\u001b[39;00m\n\u001b[0;32m     39\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((x, skip3), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32md:\\slam\\env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\slam\\env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32md:\\slam\\env\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:952\u001b[0m, in \u001b[0;36mConvTranspose2d.forward\u001b[1;34m(self, input, output_size)\u001b[0m\n\u001b[0;32m    947\u001b[0m num_spatial_dims \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[0;32m    948\u001b[0m output_padding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_padding(\n\u001b[0;32m    949\u001b[0m     \u001b[38;5;28minput\u001b[39m, output_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkernel_size,  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m    950\u001b[0m     num_spatial_dims, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m--> 952\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv_transpose2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    953\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    954\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_padding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Given transposed=1, weight of size [256, 128, 3, 3], expected input[32, 384, 14, 14] to have 256 channels, but got 384 channels instead"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = 0.0\n",
    "    for features, positions, orientations in dataloader:\n",
    "        # Forward pass\n",
    "        loc_pred, ori_pred, trans_pred = model(features)\n",
    "        \n",
    "        # Compute loss\n",
    "        loc_loss = criterion(loc_pred, positions)\n",
    "        ori_loss = criterion(ori_pred, orientations)\n",
    "        total_loss = loc_loss + ori_loss\n",
    "        \n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Track loss\n",
    "        epoch_loss += total_loss.item() * features.size(0)\n",
    "\n",
    "    # Print epoch loss\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss / len(dataset):.4f}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
